# ğŸ“” Analizador de Diario Personal con Chunking SemÃ¡ntico

Herramienta automatizada para analizar entradas de diario personal usando modelos de lenguaje locales (LM Studio). Procesa carpetas completas de diarios, extrae informaciÃ³n estructurada y divide el contenido en chunks semÃ¡nticos preparados para embeddings y RAG (Retrieval Augmented Generation).

## âœ¨ CaracterÃ­sticas

- ğŸ¤– **AnÃ¡lisis con IA Local**: Utiliza LM Studio para procesamiento privado
- ğŸ“ **Procesamiento Batch**: Analiza carpetas completas automÃ¡ticamente
- ğŸ” **DetecciÃ³n Inteligente**: Solo procesa archivos nuevos, evita duplicados
- ğŸ“Š **ExtracciÃ³n Estructurada**: Genera JSON con emociones, temas y resÃºmenes
- ğŸ“… **GestiÃ³n AutomÃ¡tica de Fechas**: Extrae y valida fechas del nombre del archivo
- ğŸ§© **Chunking SemÃ¡ntico**: Divide entradas en fragmentos coherentes (50-300 palabras)
- ğŸ·ï¸ **ClasificaciÃ³n de Chunks**: Identifica tipos (hechos, emociones, reflexiones)
- ğŸ’¾ **Doble Almacenamiento**: AnÃ¡lisis completo + chunks separados para RAG
- ğŸ›¡ï¸ **Manejo Robusto de Errores**: ValidaciÃ³n completa y mensajes claros
- ğŸ“ **Logging Detallado**: Seguimiento completo del proceso con estadÃ­sticas
- ğŸ”’ **Privacidad Total**: Todo el procesamiento es local

## ğŸ“‹ Requisitos Previos

### Software Necesario

- **Python 3.7 o superior**
- **LM Studio** instalado y en ejecuciÃ³n
  - Descarga desde: [lmstudio.ai](https://lmstudio.ai)
  - Debe estar corriendo el servidor local

### Dependencias Python

```bash
pip install lmstudio
```

## ğŸš€ InstalaciÃ³n

### 1. Clonar o descargar el proyecto

```bash
git clone https://github.com/tu-usuario/diary-analyzer.git
cd diary-analyzer
```

### 2. Crear entorno virtual (recomendado)

```bash
# Crear entorno virtual
python3 -m venv .venv

# Activar entorno virtual
# En Linux/Mac:
source .venv/bin/activate

# En Windows:
.venv\Scripts\activate
```

### 3. Instalar dependencias

```bash
pip install lmstudio
```

### 4. Crear estructura de carpetas

```bash
mkdir diarios
```

### 5. Configurar LM Studio

1. Abre LM Studio
2. Descarga el modelo: `liquidai/lfm2-2.6b-exp@f16` (o el que prefieras)
3. Inicia el servidor local (generalmente en `http://localhost:1234`)

## ğŸ“– Uso

### Uso BÃ¡sico (Procesamiento Batch con Chunking)

1. **Coloca tus archivos de diario** en la carpeta `diarios/` con el formato `dd-mm-yyyy.md`:

```
diarios/
â”œâ”€â”€ 01-12-2025.md
â”œâ”€â”€ 15-12-2025.md
â”œâ”€â”€ 20-12-2025.md
â””â”€â”€ 31-12-2025.md
```

2. **Ejecuta el script**:

```bash
python diary_analyzer.py
```

3. **Resultado**: El script generarÃ¡ dos archivos:
   - `diario.json` - AnÃ¡lisis completos de cada entrada
   - `diario_chunks.json` - Chunks semÃ¡nticos listos para embeddings

### Ejemplo de Salida

```
============================================================
INICIANDO PROCESAMIENTO BATCH DE DIARIOS
Modo: CON CHUNKING SEMÃNTICO
============================================================
Encontrados 4 archivos de diario en 'diarios'
Archivos pendientes de procesar: 2
Modo: SOLO NUEVOS
Archivos a procesar: 2
------------------------------------------------------------

[1/2] Procesando...
2025-12-31 10:15:23 - INFO - Analizando: 20-12-2025.md (entry_2025_12_20)
2025-12-31 10:15:24 - DEBUG - Texto dividido en 3 chunks
2025-12-31 10:15:24 - INFO - Creados 3 chunks para entry_2025_12_20
2025-12-31 10:15:24 - INFO - âœ“ Generados 3 chunks para entry_2025_12_20
2025-12-31 10:15:25 - INFO - âœ“ 20-12-2025.md procesado exitosamente

============================================================
RESUMEN DEL PROCESAMIENTO
============================================================
Total de archivos: 2
âœ“ Exitosos: 2
âœ— Fallidos: 0
ğŸ“¦ Chunks generados: 6

ğŸ‰ Â¡Todos los archivos procesados exitosamente!
============================================================
âœ“ Procesamiento completado: 2 archivos analizados
ğŸ“¦ Total de chunks generados: 6
============================================================
```

### ConfiguraciÃ³n Personalizada

Edita las constantes al final de `diary_analyzer.py`:

```python
if __name__ == "__main__":
    CARPETA_DIARIOS = "mis_diarios"         # Tu carpeta
    ARCHIVO_SALIDA = "diario.json"          # AnÃ¡lisis completo
    ARCHIVO_CHUNKS = "diario_chunks.json"   # Chunks para RAG
    MODELO_LLM = "mistral-7b-instruct"      # Modelo diferente
    FORZAR_REPROCESAR = False               # Reprocesar todo
    GENERAR_CHUNKS = True                   # Activar/desactivar chunking
```

### Desactivar Chunking

Si solo quieres el anÃ¡lisis sin chunks:

```python
GENERAR_CHUNKS = False
```

### Uso como MÃ³dulo

```python
from diary_analyzer import procesar_carpeta_diarios

# Procesar con chunking
estadisticas = procesar_carpeta_diarios(
    carpeta="diarios",
    ruta_salida="diario.json",
    ruta_chunks="chunks.json",
    generar_chunks=True
)

print(f"Chunks generados: {estadisticas['chunks_generados']}")
```

## ğŸ“„ Formatos de Salida

### 1. Archivo de AnÃ¡lisis (`diario.json`)

AnÃ¡lisis completo de cada entrada con texto original:

```json
[
  {
    "id": "entry_2025_12_15",
    "fecha": "15-12-2025",
    "raw_text": "# 15 de Diciembre\n\nHoy fue un dÃ­a interesante...",
    "summary": "ReuniÃ³n productiva sobre proyecto con MarÃ­a...",
    "emotions": ["ansioso", "emocionado"],
    "topics": ["trabajo", "programaciÃ³n", "viajes"],
    "people": ["MarÃ­a", "Juan"],
    "intensity": "media",
    "word_count": 342,
    "char_count": 1876,
    "chunk_count": 3
  }
]
```

#### Campos del AnÃ¡lisis

- **id**: Identificador Ãºnico (`entry_yyyy_mm_dd`)
- **fecha**: Fecha en formato `dd-mm-yyyy`
- **raw_text**: Texto completo original del diario
- **summary**: Resumen neutral en mÃ¡ximo 3 lÃ­neas
- **emotions**: Lista de emociones detectadas
- **topics**: Temas principales discutidos
- **people**: Personas mencionadas (null si no hay)
- **intensity**: Intensidad emocional ("baja", "media", "alta")
- **word_count**: Cantidad de palabras
- **char_count**: Cantidad de caracteres
- **chunk_count**: NÃºmero de chunks generados

### 2. Archivo de Chunks (`diario_chunks.json`)

Chunks semÃ¡nticos enriquecidos, listos para convertir en embeddings:

```json
[
  {
    "chunk_id": "entry_2025_12_15_chunk_0",
    "entry_id": "entry_2025_12_15",
    "index": 0,
    "text": "Hoy fue un dÃ­a interesante. Me reunÃ­ con MarÃ­a para discutir el proyecto...",
    "word_count": 156,
    "char_count": 823,
    "type": "hechos",
    "metadata": {
      "date": "15-12-2025",
      "emotions": ["ansioso", "emocionado"],
      "topics": ["trabajo", "programaciÃ³n"],
      "intensity": "media",
      "people": ["MarÃ­a", "Juan"]
    }
  },
  {
    "chunk_id": "entry_2025_12_15_chunk_1",
    "entry_id": "entry_2025_12_15",
    "index": 1,
    "text": "Me sentÃ­ un poco ansioso al principio...",
    "word_count": 98,
    "char_count": 512,
    "type": "emociones",
    "metadata": {
      "date": "15-12-2025",
      "emotions": ["ansioso", "emocionado"],
      "topics": ["trabajo", "programaciÃ³n"],
      "intensity": "media",
      "people": ["MarÃ­a", "Juan"]
    }
  }
]
```

#### Campos de Chunks

- **chunk_id**: ID Ãºnico del chunk
- **entry_id**: ID de la entrada padre
- **index**: PosiciÃ³n del chunk (0, 1, 2...)
- **text**: Contenido textual del chunk (100-300 palabras)
- **word_count**: Palabras en el chunk
- **char_count**: Caracteres en el chunk
- **type**: Tipo de contenido ("hechos", "emociones", "reflexion", "mixto")
- **metadata**: InformaciÃ³n contextual heredada del anÃ¡lisis

### Tipos de Chunks

El sistema clasifica automÃ¡ticamente cada chunk:

- **hechos**: Eventos, acciones, descripciÃ³n de actividades
- **emociones**: Sentimientos, estados emocionales explÃ­citos
- **reflexion**: Pensamientos, aprendizajes, introspecciÃ³n
- **mixto**: CombinaciÃ³n de varios tipos

## ğŸ”® PrÃ³ximos Pasos: Embeddings y RAG

### Â¿Por quÃ© Chunking?

El chunking prepara tus diarios para:

1. **BÃºsqueda SemÃ¡ntica**: Encontrar entradas por significado, no solo palabras
2. **Chatbot de Diario**: Conversar con tus memorias usando IA
3. **AnÃ¡lisis de Patrones**: Detectar tendencias emocionales a lo largo del tiempo
4. **Recomendaciones**: "DÃ­as similares a hoy", "Cuando te sentÃ­as asÃ­..."

### Roadmap de Embeddings (Futuro)

#### Fase 1: Generar Embeddings

```python
# FUTURO - No implementado aÃºn
from sentence_transformers import SentenceTransformer
import json

# 1. Cargar modelo de embeddings (pequeÃ±o y rÃ¡pido)
model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
# o para mejor calidad (mÃ¡s pesado):
# model = SentenceTransformer('BAAI/bge-small-en-v1.5')

# 2. Cargar chunks
with open('diario_chunks.json', 'r') as f:
    chunks = json.load(f)

# 3. Generar embeddings
for chunk in chunks:
    embedding = model.encode(chunk['text'])
    chunk['embedding'] = embedding.tolist()

# 4. Guardar chunks con embeddings
with open('diario_chunks_embedded.json', 'w') as f:
    json.dump(chunks, f)
```

**Modelos recomendados para espaÃ±ol**:
- `hiiamsid/sentence_similarity_spanish_es` (ligero)
- `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2` (multilingÃ¼e)

#### Fase 2: Crear Base de Datos Vectorial

```python
# FUTURO - No implementado aÃºn
import faiss
import numpy as np

# OpciÃ³n A: FAISS (mÃ¡s simple, local)
embeddings = np.array([chunk['embedding'] for chunk in chunks])
index = faiss.IndexFlatL2(embeddings.shape[1])
index.add(embeddings)
faiss.write_index(index, 'diario_vectors.index')

# OpciÃ³n B: ChromaDB (mÃ¡s features)
import chromadb

client = chromadb.Client()
collection = client.create_collection("diario")

for chunk in chunks:
    collection.add(
        embeddings=[chunk['embedding']],
        documents=[chunk['text']],
        metadatas=[chunk['metadata']],
        ids=[chunk['chunk_id']]
    )
```

#### Fase 3: BÃºsqueda SemÃ¡ntica

```python
# FUTURO - Ejemplo de bÃºsqueda
def buscar_en_diario(query, top_k=5):
    # Convertir query a embedding
    query_embedding = model.encode(query)
    
    # Buscar similares
    distances, indices = index.search(
        np.array([query_embedding]), 
        top_k
    )
    
    # Devolver chunks relevantes
    resultados = [chunks[i] for i in indices[0]]
    return resultados

# Uso
resultados = buscar_en_diario("dÃ­as donde me sentÃ­ ansioso")
for chunk in resultados:
    print(f"Fecha: {chunk['metadata']['date']}")
    print(f"Texto: {chunk['text'][:100]}...")
```

#### Fase 4: RAG con Modelo Local (8B)

```python
# FUTURO - Chatbot con memoria
import lmstudio as lms

def chatear_con_diario(pregunta):
    # 1. Buscar contexto relevante
    chunks_relevantes = buscar_en_diario(pregunta, top_k=3)
    contexto = "\n\n".join([c['text'] for c in chunks_relevantes])
    
    # 2. Construir prompt con contexto
    prompt = f"""
    BasÃ¡ndote SOLO en estos fragmentos de mi diario:
    
    {contexto}
    
    Responde a mi pregunta: {pregunta}
    
    SÃ© empÃ¡tico y personal. Usa "tÃº" para dirigirte a mÃ­.
    """
    
    # 3. Usar modelo local (ej: Llama-3-8B, Mistral-7B)
    with lms.Client() as client:
        model = client.llm.model("llama-3-8b-instruct")
        response = model.respond(prompt)
        return response.content

# Uso
respuesta = chatear_con_diario("Â¿CÃ³mo me sentÃ­ en diciembre?")
print(respuesta)
```

### Modelos Recomendados para RAG (8B locales)

1. **Llama 3.1 8B Instruct** - Excelente balance calidad/velocidad
2. **Mistral 7B Instruct** - Muy rÃ¡pido, buena calidad
3. **Phi-3 Medium (14B)** - Si tienes mÃ¡s RAM
4. **Gemma 2 9B** - Alternativa de Google

### Herramientas para el Pipeline Completo

```bash
# Instalar dependencias futuras
pip install sentence-transformers
pip install faiss-cpu  # o faiss-gpu si tienes NVIDIA
pip install chromadb   # alternativa a FAISS
pip install numpy
```

### Arquitectura Futura del Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  diarios/*.md   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ diary_analyzer  â”‚ â† ACTUAL
â”‚  .py (LLM 2.6B) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â–º diario.json (anÃ¡lisis)
         â”‚
         â””â”€â–º diario_chunks.json (chunks)
                    â”‚
                    â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  embedding.py    â”‚ â† FUTURO
         â”‚  (all-MiniLM)    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â””â”€â–º diario_vectors.db
                              â”‚
                              â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚   rag_chat.py    â”‚ â† FUTURO
                   â”‚  (Llama 3 8B)    â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Casos de Uso Futuros

1. **BÃºsqueda Contextual**:
   ```
   Usuario: "dÃ­as donde estuve con MarÃ­a"
   Sistema: [muestra chunks relevantes con fechas]
   ```

2. **AnÃ¡lisis Temporal**:
   ```
   Usuario: "Â¿cÃ³mo cambiÃ³ mi ansiedad este mes?"
   Sistema: [busca chunks de "ansiedad", analiza tendencia]
   ```

3. **ConversaciÃ³n Natural**:
   ```
   Usuario: "dame consejos basados en cÃ³mo superÃ© problemas antes"
   Sistema: [busca entradas de superaciÃ³n, genera consejo]
   ```

4. **ReflexiÃ³n Asistida**:
   ```
   Usuario: "Â¿quÃ© he aprendido sobre el trabajo?"
   Sistema: [busca reflexiones sobre trabajo, sintetiza]
   ```

## ğŸ”§ ConfiguraciÃ³n Avanzada

### Ajustar TamaÃ±o de Chunks

En `diary_analyzer.py`, modifica la funciÃ³n `dividir_en_chunks_semanticos`:

```python
chunks = dividir_en_chunks_semanticos(
    texto,
    min_palabras=50,   # MÃ­nimo por chunk
    max_palabras=200   # MÃ¡ximo por chunk
)
```

**Recomendaciones**:
- **Para embeddings pequeÃ±os** (all-MiniLM): 100-300 palabras
- **Para embeddings grandes** (bge-large): 200-500 palabras
- **Para textos cortos**: reduce a 50-150 palabras

### Personalizar ClasificaciÃ³n de Chunks

Edita las palabras clave en `clasificar_tipo_chunk()`:

```python
# Agregar tus propias palabras indicadoras
palabras_emocionales = [
    'sentÃ­', 'siento', 'feliz', 'triste',
    # Agrega mÃ¡s segÃºn tu vocabulario
]
```

## ğŸ› SoluciÃ³n de Problemas

### No se generan chunks

**Causa**: `GENERAR_CHUNKS = False` o texto muy corto

**SoluciÃ³n**: 
- Verifica que `GENERAR_CHUNKS = True`
- AsegÃºrate de que las entradas tengan al menos 100 palabras

### Chunks muy grandes o pequeÃ±os

**SoluciÃ³n**: Ajusta los parÃ¡metros de `dividir_en_chunks_semanticos()`

### Tipo de chunk siempre "mixto"

**Causa**: Las palabras clave no coinciden con tu vocabulario

**SoluciÃ³n**: Personaliza las listas de palabras en `clasificar_tipo_chunk()`

### Archivo chunks.json muy grande

**Normal**: Si tienes muchas entradas, considera:
- Usar base de datos vectorial en lugar de JSON
- Comprimir el archivo: `gzip diario_chunks.json`

## ğŸ“ Estructura del Proyecto

```
diary-analyzer/
â”œâ”€â”€ diary_analyzer.py           # Script principal
â”œâ”€â”€ README.md                   # Esta documentaciÃ³n
â”œâ”€â”€ requirements.txt            # Dependencias
â”œâ”€â”€ .venv/                      # Entorno virtual
â”œâ”€â”€ diarios/                    # Carpeta con archivos .md
â”‚   â”œâ”€â”€ 01-12-2025.md
â”‚   â”œâ”€â”€ 15-12-2025.md
â”‚   â””â”€â”€ 31-12-2025.md
â”œâ”€â”€ diario.json                # AnÃ¡lisis completos (generado)
â””â”€â”€ diario_chunks.json         # Chunks semÃ¡nticos (generado)

# Futuros archivos (no implementados aÃºn)
â”œâ”€â”€ embedding_generator.py     # FUTURO: Generar embeddings
â”œâ”€â”€ vector_db.py              # FUTURO: GestiÃ³n de vectores
â”œâ”€â”€ rag_chat.py               # FUTURO: Chatbot con RAG
â””â”€â”€ diario_vectors.db         # FUTURO: Base de datos vectorial
```

## ğŸ”’ Privacidad y Seguridad

- âœ… Todo el procesamiento es **100% local**
- âœ… No se envÃ­an datos a servicios externos
- âœ… Tus diarios permanecen en tu computadora
- âœ… Los embeddings (futuros) tambiÃ©n serÃ¡n locales
- âœ… Sin conexiÃ³n a internet requerida (excepto instalaciÃ³n inicial)

## ğŸ¤ Contribuciones

Las contribuciones son bienvenidas, especialmente:

- [ ] ImplementaciÃ³n de generaciÃ³n de embeddings
- [ ] IntegraciÃ³n con ChromaDB/FAISS
- [ ] Sistema RAG completo
- [ ] Interfaz de chat
- [ ] Mejoras en clasificaciÃ³n de chunks

Por favor:

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/embeddings`)
3. Commit tus cambios (`git commit -m 'feat: agregar embeddings'`)
4. Push a la rama (`git push origin feature/embeddings`)
5. Abre un Pull Request

## ğŸ“ Ejemplos de Uso

### Analizar EstadÃ­sticas de Chunks

```python
import json
from collections import Counter

# Cargar chunks
with open('diario_chunks.json') as f:
    chunks = json.load(f)

# Tipos de chunks mÃ¡s comunes
tipos = [c['type'] for c in chunks]
print(Counter(tipos))

# Chunks por entrada
from collections import defaultdict
chunks_por_entrada = defaultdict(int)
for c in chunks:
    chunks_por_entrada[c['entry_id']] += 1

print(f"Promedio de chunks por entrada: {sum(chunks_por_entrada.values()) / len(chunks_por_entrada):.2f}")
```

### Buscar Chunks por Tipo

```python
def buscar_por_tipo(tipo, limite=5):
    with open('diario_chunks.json') as f:
        chunks = json.load(f)
    
    resultado = [c for c in chunks if c['type'] == tipo]
    return resultado[:limite]

# Ver chunks de emociones
emociones = buscar_por_tipo('emociones')
for chunk in emociones:
    print(f"Fecha: {chunk['metadata']['date']}")
    print(f"Texto: {chunk['text'][:100]}...")
    print()
```

### Exportar Chunks a CSV

```python
import csv

def exportar_chunks_csv(archivo_salida='chunks.csv'):
    with open('diario_chunks.json') as f:
        chunks = json.load(f)
    
    with open(archivo_salida, 'w', newline='', encoding='utf-8') as f:
        campos = ['chunk_id', 'entry_id', 'date', 'type', 'text', 'word_count']
        writer = csv.DictWriter(f, fieldnames=campos)
        
        writer.writeheader()
        for chunk in chunks:
            writer.writerow({
                'chunk_id': chunk['chunk_id'],
                'entry_id': chunk['entry_id'],
                'date': chunk['metadata']['date'],
                'type': chunk['type'],
                'text': chunk['text'],
                'word_count': chunk['word_count']
            })

exportar_chunks_csv()
```

## ğŸ—ºï¸ Roadmap

### VersiÃ³n Actual (2.0)
- [x] Procesamiento batch de carpetas
- [x] Chunking semÃ¡ntico automÃ¡tico
- [x] ClasificaciÃ³n de tipos de chunks
- [x] Doble almacenamiento (anÃ¡lisis + chunks)

### PrÃ³ximas Versiones

**v2.1 - Embeddings** (prÃ³ximo)
- [ ] Script para generar embeddings
- [ ] IntegraciÃ³n con sentence-transformers
- [ ] Soporte para modelos en espaÃ±ol
- [ ] ActualizaciÃ³n incremental de embeddings

**v2.2 - Base de Datos Vectorial**
- [ ] IntegraciÃ³n con FAISS
- [ ] Alternativa con ChromaDB
- [ ] BÃºsqueda por similitud semÃ¡ntica
- [ ] API de consulta

**v3.0 - RAG Completo**
- [ ] Chatbot conversacional
- [ ] IntegraciÃ³n con modelos 8B locales
- [ ] Memoria conversacional
- [ ] Interfaz de chat (CLI)

**v3.1 - Interfaz GrÃ¡fica**
- [ ] GUI con Streamlit/Gradio
- [ ] VisualizaciÃ³n de embeddings (t-SNE/UMAP)
- [ ] GrÃ¡ficos de emociones temporales
- [ ] Dashboard interactivo

**Futuro**
- [ ] AnÃ¡lisis de patrones y tendencias
- [ ] Recomendaciones basadas en contexto
- [ ] ExportaciÃ³n a mÃºltiples formatos
- [ ] App mÃ³vil (opcional)

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Ver archivo `LICENSE` para mÃ¡s detalles.

## ğŸ‘¤ Autor

**Fabri**

- GitHub: [@tu-usuario](https://github.com/tu-usuario)

## ğŸ™ Agradecimientos

- [LM Studio](https://lmstudio.ai) por la plataforma local de LLMs
- [Liquid AI](https://liquid.ai) por el modelo LFM
- [Sentence Transformers](https://www.sbert.net/) por los embeddings
- La comunidad de RAG y bÃºsqueda semÃ¡ntica

## ğŸ“ Soporte

Si tienes problemas o preguntas:

1. Revisa la secciÃ³n de [SoluciÃ³n de Problemas](#-soluciÃ³n-de-problemas)
2. Busca en [Issues](https://github.com/tu-usuario/diary-analyzer/issues)
3. Abre un nuevo issue con:
   - VersiÃ³n de Python
   - Sistema operativo
   - Mensaje de error completo
   - Logs relevantes
   - Ejemplo del archivo de diario (si es posible)

## ğŸ”„ Changelog

### v2.0.0 (2025-01-01) - ACTUAL
- âœ¨ **Chunking semÃ¡ntico automÃ¡tico**
- âœ¨ DivisiÃ³n inteligente por pÃ¡rrafos (100-300 palabras)
- âœ¨ ClasificaciÃ³n automÃ¡tica de chunks (hechos/emociones/reflexiÃ³n)
- âœ¨ Doble almacenamiento: anÃ¡lisis + chunks
- âœ¨ IDs Ãºnicos para entries y chunks
- âœ¨ Metadata enriquecida en cada chunk
- âœ¨ Campo `raw_text` en anÃ¡lisis principal
- ğŸ“ DocumentaciÃ³n completa sobre embeddings futuros
- ğŸ“ Roadmap detallado para RAG

### v1.1.0 (2024-12-25)
- âœ¨ Procesamiento batch de carpetas completas
- âœ¨ DetecciÃ³n automÃ¡tica de archivos ya procesados
- âœ¨ ExtracciÃ³n y validaciÃ³n de fechas desde nombres de archivo
- âœ¨ Sistema de estadÃ­sticas y resumen
- âœ¨ ValidaciÃ³n de formato de nombres de archivo
- ğŸ› Mejoras en manejo de errores
- ğŸ“ Logging mÃ¡s detallado

### v1.0.0 (2024-12-15)
- ğŸ‰ VersiÃ³n inicial
- âœ¨ AnÃ¡lisis individual de archivos
- âœ¨ IntegraciÃ³n con LM Studio
- âœ¨ ExtracciÃ³n de emociones, temas y personas

---

**Â¿Te resultÃ³ Ãºtil este proyecto? Â¡Dale una â­ en GitHub!**